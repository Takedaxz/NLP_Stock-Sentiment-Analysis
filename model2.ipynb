{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bac48f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\acer\\anaconda3\\envs\\NLP2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AdamW,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1899de54",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0b80b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "TICKERS        = ['NVDA','AAPL','AMZN','GOOGL','MSFT','META','TSLA']\n",
    "START_DATE     = '2009-07-01'\n",
    "END_DATE       = '2025-05-03'\n",
    "TRANSFORMER    = 'yiyanghkust/finbert-tone'\n",
    "MAX_LENGTH     = 256\n",
    "BATCH_SIZE     = 16\n",
    "LR             = 2e-5\n",
    "EPOCHS         = 10\n",
    "TH_UP, TH_DOWN = 0.005, -0.005\n",
    "TICKER_EMB_DIM = 16\n",
    "PATIENCE       = 3\n",
    "OUTPUT_DIR     = './model_output'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0f7b268",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nvda  = pd.read_pickle('data/NVDA_article_sentiments.pkl')\n",
    "df_appl  = pd.read_pickle('data/AAPL_article_sentiments.pkl')\n",
    "df_amzn  = pd.read_pickle('data/AMZN_article_sentiments.pkl')\n",
    "df_googl = pd.read_pickle('data/GOOGL_article_sentiments.pkl')\n",
    "df_msft = pd.read_pickle('data/MSFT_article_sentiments.pkl')\n",
    "df_meta = pd.read_pickle('data/META_article_sentiments.pkl')\n",
    "df_tsla = pd.read_pickle('data/TSLA_article_sentiments.pkl')\n",
    "df_news  = pd.concat([df_nvda, df_appl, df_amzn, df_googl,df_msft,df_meta,df_tsla], axis=0, ignore_index=True)\n",
    "\n",
    "df_news['date'] = pd.to_datetime(df_news['publish_datetime']).dt.date\n",
    "# เราจะใช้เฉพาะ title+body_text concatenation แค่ครั้งเดียว\n",
    "agg_news = df_news.groupby(['ticker','date']).agg({\n",
    "    'title':     lambda ts: ' '.join(ts),\n",
    "    'body_text': lambda bs: ' '.join(bs)\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d46767e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2021-10-28</td>\n",
       "      <td>Global stocks fall, U.S. dollar climbs on infl...</td>\n",
       "      <td>By Chibuike Oguh NEW YORK (Reuters) -Global eq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2021-10-29</td>\n",
       "      <td>Apple objects to links to outside payments ahe...</td>\n",
       "      <td>By Stephen Nellis (Reuters) - Apple Inc (NASDA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2021-10-30</td>\n",
       "      <td>Cuomo attorney says sheriff leaked grand jury ...</td>\n",
       "      <td>By Tim Reid LOS ANGELES (Reuters) - An attorne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2021-10-31</td>\n",
       "      <td>Top 5 Things to Watch in Markets in the Week A...</td>\n",
       "      <td>by Daniel Shvartsman Despite high-profile earn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2021-11-01</td>\n",
       "      <td>Apple cuts iPad production to feed chips to iP...</td>\n",
       "      <td>(Reuters) - Apple Inc (NASDAQ: ) has cut back ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9154</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2025-04-30</td>\n",
       "      <td>Tesla stock gains after denying CEO search rep...</td>\n",
       "      <td>Investing.com -- Tesla (NASDAQ: ) denied an ov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9155</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2025-05-01</td>\n",
       "      <td>Analysis-Tesla without Musk? Board faces uniqu...</td>\n",
       "      <td>By Rachael Levy, Abhirup Roy, Isla Binnie (Reu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9156</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2025-05-02</td>\n",
       "      <td>Tesla’s Italy car registrations rise in April,...</td>\n",
       "      <td>Investing.com -- Tesla (NASDAQ: ) has seen an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9157</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2025-05-03</td>\n",
       "      <td>Can Tesla help the U.S. catch up to China in t...</td>\n",
       "      <td>Investing.com -- Tesla (NASDAQ: ) could play a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9158</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>2025-05-05</td>\n",
       "      <td>Wall Street analyst says Tesla and its mission...</td>\n",
       "      <td>Investing.com -- Tesla’s near $1 trillion valu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9159 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ticker        date                                              title  \\\n",
       "0      AAPL  2021-10-28  Global stocks fall, U.S. dollar climbs on infl...   \n",
       "1      AAPL  2021-10-29  Apple objects to links to outside payments ahe...   \n",
       "2      AAPL  2021-10-30  Cuomo attorney says sheriff leaked grand jury ...   \n",
       "3      AAPL  2021-10-31  Top 5 Things to Watch in Markets in the Week A...   \n",
       "4      AAPL  2021-11-01  Apple cuts iPad production to feed chips to iP...   \n",
       "...     ...         ...                                                ...   \n",
       "9154   TSLA  2025-04-30  Tesla stock gains after denying CEO search rep...   \n",
       "9155   TSLA  2025-05-01  Analysis-Tesla without Musk? Board faces uniqu...   \n",
       "9156   TSLA  2025-05-02  Tesla’s Italy car registrations rise in April,...   \n",
       "9157   TSLA  2025-05-03  Can Tesla help the U.S. catch up to China in t...   \n",
       "9158   TSLA  2025-05-05  Wall Street analyst says Tesla and its mission...   \n",
       "\n",
       "                                              body_text  \n",
       "0     By Chibuike Oguh NEW YORK (Reuters) -Global eq...  \n",
       "1     By Stephen Nellis (Reuters) - Apple Inc (NASDA...  \n",
       "2     By Tim Reid LOS ANGELES (Reuters) - An attorne...  \n",
       "3     by Daniel Shvartsman Despite high-profile earn...  \n",
       "4     (Reuters) - Apple Inc (NASDAQ: ) has cut back ...  \n",
       "...                                                 ...  \n",
       "9154  Investing.com -- Tesla (NASDAQ: ) denied an ov...  \n",
       "9155  By Rachael Levy, Abhirup Roy, Isla Binnie (Reu...  \n",
       "9156  Investing.com -- Tesla (NASDAQ: ) has seen an ...  \n",
       "9157  Investing.com -- Tesla (NASDAQ: ) could play a...  \n",
       "9158  Investing.com -- Tesla’s near $1 trillion valu...  \n",
       "\n",
       "[9159 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "196e32a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['NVDA']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['AAPL']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['AMZN']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['GOOGL']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['MSFT']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['META']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['TSLA']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "price_dfs = []\n",
    "for tk in TICKERS:\n",
    "    p = yf.download(tk, start=START_DATE, end=END_DATE, auto_adjust=False)\n",
    "    # Flatten columns if MultiIndex\n",
    "    if isinstance(p.columns, pd.MultiIndex):\n",
    "        p.columns = p.columns.get_level_values(0)\n",
    "    p = p.rename(columns={\n",
    "        'Open':'open_t','High':'high_t','Low':'low_t',\n",
    "        'Close':'close_t','Volume':'volume_t'\n",
    "    })\n",
    "    # MA, Momentum, ATR, RSI, MACD_hist\n",
    "    p['MA5_t']       = p['close_t'].rolling(5).mean()\n",
    "    p['MA10_t']      = p['close_t'].rolling(10).mean()\n",
    "    p['momentum5_t'] = p['close_t'] - p['close_t'].shift(5)\n",
    "    hl = p['high_t'] - p['low_t']\n",
    "    hc = (p['high_t'] - p['close_t'].shift(1)).abs()\n",
    "    lc = (p['low_t']  - p['close_t'].shift(1)).abs()\n",
    "    tr = pd.concat([hl,hc,lc],axis=1).max(axis=1)\n",
    "    p['ATR14_t']     = tr.rolling(14).mean()\n",
    "    delta = p['close_t'].diff()\n",
    "    gain  = delta.clip(lower=0); loss = -delta.clip(upper=0)\n",
    "    avg_g = gain.rolling(14).mean(); avg_l = loss.rolling(14).mean()\n",
    "    rs    = avg_g/avg_l\n",
    "    p['RSI14_t']     = 100 - (100/(1+rs))\n",
    "    ema12 = p['close_t'].ewm(span=12,adjust=False).mean()\n",
    "    ema26 = p['close_t'].ewm(span=26,adjust=False).mean()\n",
    "    macd  = ema12 - ema26\n",
    "    signal= macd.ewm(span=9,adjust=False).mean()\n",
    "    p['MACD_hist_t'] = macd - signal\n",
    "    # target label\n",
    "    p['close_t+1']        = p['close_t'].shift(-1)\n",
    "    p['future_return_1d'] = (p['close_t+1'] - p['close_t'])/p['close_t']\n",
    "    p['label'] = p['future_return_1d'].apply(\n",
    "        lambda r: 2 if r>TH_UP else 0 if r<TH_DOWN else 1\n",
    "    )\n",
    "    req = ['open_t','high_t','low_t','close_t','volume_t',\n",
    "           'MA5_t','MA10_t','momentum5_t','ATR14_t','RSI14_t','MACD_hist_t',\n",
    "           'future_return_1d','label']\n",
    "    p = p.dropna(subset=req)\n",
    "    p = p.reset_index().rename(columns={'Date':'date'})\n",
    "    p['date']   = p['date'].dt.date\n",
    "    p['ticker'] = tk\n",
    "    price_dfs.append(p)\n",
    "\n",
    "\n",
    "df_price = pd.concat(price_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b9aa694",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(agg_news, df_price, on=['ticker','date'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ba36ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>body_text</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>close_t</th>\n",
       "      <th>high_t</th>\n",
       "      <th>low_t</th>\n",
       "      <th>open_t</th>\n",
       "      <th>volume_t</th>\n",
       "      <th>MA5_t</th>\n",
       "      <th>MA10_t</th>\n",
       "      <th>momentum5_t</th>\n",
       "      <th>ATR14_t</th>\n",
       "      <th>RSI14_t</th>\n",
       "      <th>MACD_hist_t</th>\n",
       "      <th>close_t+1</th>\n",
       "      <th>future_return_1d</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ticker, date, title, body_text, Adj Close, close_t, high_t, low_t, open_t, volume_t, MA5_t, MA10_t, momentum5_t, ATR14_t, RSI14_t, MACD_hist_t, close_t+1, future_return_1d, label]\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1691745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ticker_idx'] = df['ticker'].astype('category').cat.codes\n",
    "num_tickers     = df['ticker_idx'].nunique()\n",
    "\n",
    "# set device (GPU if available, else CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# tabular cols\n",
    "tab_cols = ['open_t','high_t','low_t','close_t','volume_t',\n",
    "            'MA5_t','MA10_t','momentum5_t','ATR14_t','RSI14_t','MACD_hist_t']\n",
    "\n",
    "# scale tabular\n",
    "scaler = StandardScaler()\n",
    "df[tab_cols] = scaler.fit_transform(df[tab_cols])\n",
    "\n",
    "# pre-tokenize ALL text (title+body) เพื่อไม่ tokenize ซ้ำในแต่ละ batch\n",
    "tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER, use_fast=True)\n",
    "texts = (df['title'] + ' ' + df['body_text']).tolist()\n",
    "enc = tokenizer(texts, padding=True, truncation=True,\n",
    "                max_length=MAX_LENGTH, return_tensors='pt')\n",
    "\n",
    "df['input_ids']      = enc['input_ids'].tolist()\n",
    "df['attention_mask']= enc['attention_mask'].tolist()\n",
    "\n",
    "# train/test split ตามวัน (time-series)\n",
    "df = df.sort_values(['date','ticker']).reset_index(drop=True)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, shuffle=False)\n",
    "\n",
    "# compute class weights\n",
    "class_counts = train_df['label'].value_counts().sort_index()\n",
    "class_weights= torch.tensor(class_counts.sum()/class_counts.values,\n",
    "                            dtype=torch.float).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f47bcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsStockDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, i):\n",
    "        row = self.df.iloc[i]\n",
    "        return {\n",
    "            'input_ids':      torch.tensor(row['input_ids'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(row['attention_mask'], dtype=torch.long),\n",
    "            'tabular':        torch.tensor(row[tab_cols].values, dtype=torch.float),\n",
    "            'ticker_idx':     torch.tensor(row['ticker_idx'], dtype=torch.long),\n",
    "            'labels':         torch.tensor(row['label'], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_ds = NewsStockDataset(train_df)\n",
    "test_ds  = NewsStockDataset(test_df)\n",
    "\n",
    "# weighted sampler to combat class imbalance\n",
    "sample_weights = train_df['label'].map(\n",
    "    lambda x: class_counts.sum()/class_counts[x]\n",
    ").values\n",
    "sampler = WeightedRandomSampler(sample_weights,\n",
    "                                num_samples=len(sample_weights),\n",
    "                                replacement=True)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
    "                      sampler=sampler, num_workers=2)\n",
    "test_dl  = DataLoader(test_ds,  batch_size=BATCH_SIZE,\n",
    "                      shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93b57ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\acer\\anaconda3\\envs\\NLP2\\Lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_23256\\1280481820.py:43: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler    = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, transformer_name, tab_dim, num_tickers, ticker_emb_dim):\n",
    "        super().__init__()\n",
    "        # text encoder\n",
    "        self.text_enc   = AutoModel.from_pretrained(transformer_name)\n",
    "        self.text_enc.gradient_checkpointing_enable()   # ลด memory\n",
    "        txt_dim         = self.text_enc.config.hidden_size\n",
    "        # tabular MLP\n",
    "        self.tab_mlp    = nn.Sequential(\n",
    "            nn.Linear(tab_dim, 64), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),      nn.ReLU(), nn.Dropout(0.2)\n",
    "        )\n",
    "        # ticker embedding\n",
    "        self.ticker_emb = nn.Embedding(num_tickers, ticker_emb_dim)\n",
    "        # classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(txt_dim+32+ticker_emb_dim, 128),\n",
    "            nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(128, 3)\n",
    "        )\n",
    "    def forward(self, input_ids, attention_mask, tabular, ticker_idx):\n",
    "        txt = self.text_enc(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        h_text = txt.pooler_output\n",
    "        h_tab  = self.tab_mlp(tabular)\n",
    "        h_tk   = self.ticker_emb(ticker_idx)\n",
    "        x      = torch.cat([h_text,h_tab,h_tk], dim=1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model  = MultiModalModel(TRANSFORMER, len(tab_cols),\n",
    "             num_tickers, TICKER_EMB_DIM).to(device)\n",
    "\n",
    "# optimizer + scheduler + loss + scaler\n",
    "optimizer = AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "total_steps = EPOCHS * len(train_dl)\n",
    "scheduler   = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1*total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "scaler    = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85865c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1, epochs_no_improve = 0, 0\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train(); total_loss = 0\n",
    "    for batch in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            logits = model(\n",
    "                batch['input_ids'].to(device),\n",
    "                batch['attention_mask'].to(device),\n",
    "                batch['tabular'].to(device),\n",
    "                batch['ticker_idx'].to(device)\n",
    "            )\n",
    "            loss = criterion(logits, batch['labels'].to(device))\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    # validation\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dl:\n",
    "            logits = model(\n",
    "                batch['input_ids'].to(device),\n",
    "                batch['attention_mask'].to(device),\n",
    "                batch['tabular'].to(device),\n",
    "                batch['ticker_idx'].to(device)\n",
    "            )\n",
    "            preds = logits.argmax(dim=1).cpu().numpy()\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(batch['labels'].numpy())\n",
    "    all_preds  = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1  = f1_score(all_labels, all_preds, average='weighted')\n",
    "    print(f\"Epoch {epoch} — train_loss: {total_loss/len(train_dl):.4f} — val_acc: {acc:.4f} — val_f1: {f1:.4f}\")\n",
    "    # early stopping\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), os.path.join(OUTPUT_DIR,'best.pt'))\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(\"Early stopping\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292ba466",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(OUTPUT_DIR,'best.pt')))\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
